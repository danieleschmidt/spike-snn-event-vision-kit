"""
High-Performance Neuromorphic Vision System Integration Demo.

This demonstration showcases the complete cutting-edge performance optimization
and auto-scaling system for neuromorphic vision processing, featuring:

- Multi-GPU distributed processing with CUDA kernel optimization
- Intelligent multi-level caching with pattern recognition
- High-performance async event processing with lock-free data structures  
- Dynamic auto-scaling with intelligent resource prediction
- Advanced monitoring with sub-millisecond accuracy tracking
- Comprehensive performance validation

Capable of processing millions of events per second with sub-millisecond latency.
"""

import asyncio
import time
import threading
import logging
from typing import Dict, List, Any
from pathlib import Path
import json
import numpy as np

from src.spike_snn_event.gpu_distributed_processor import (
    get_distributed_gpu_processor,
    ProcessingTask,
    NeuromorphicEvent as GPUEvent
)
from src.spike_snn_event.async_event_processor import (
    get_async_event_pipeline,
    NeuromorphicEvent,
    EventPriority
)
from src.spike_snn_event.intelligent_cache_system import get_intelligent_cache
from src.spike_snn_event.intelligent_autoscaler import get_intelligent_autoscaler
from src.spike_snn_event.advanced_telemetry import get_telemetry_system
from performance_benchmark_suite import PerformanceBenchmarkSuite, BenchmarkConfig


class HighPerformanceNeuromorphicDemo:
    """Comprehensive demonstration of the high-performance neuromorphic vision system."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # System components
        self.gpu_processor = None
        self.async_pipeline = None
        self.cache_system = None
        self.autoscaler = None
        self.telemetry = None
        
        # Demo configuration
        self.demo_duration = 120.0  # 2 minutes
        self.target_throughput = 1_000_000  # 1M events/sec
        self.event_resolution = (1280, 720)  # HD resolution
        
        # Performance tracking
        self.demo_stats = {
            'events_generated': 0,
            'events_processed': 0,
            'peak_throughput_eps': 0.0,
            'average_latency_ms': 0.0,
            'system_health_score': 0.0,
            'scaling_actions': 0,
            'cache_hit_rate': 0.0
        }
        
    async def initialize_system(self):
        """Initialize all system components."""
        self.logger.info("üöÄ Initializing High-Performance Neuromorphic Vision System...")
        
        try:
            # 1. GPU Distributed Processor
            self.logger.info("  üìä Initializing GPU distributed processor...")
            self.gpu_processor = get_distributed_gpu_processor()
            self.gpu_processor.start_processing()
            
            # 2. Async Event Pipeline  
            self.logger.info("  üîÑ Starting async event processing pipeline...")
            self.async_pipeline = get_async_event_pipeline()
            await self.async_pipeline.start_pipeline()
            
            # 3. Intelligent Cache System
            self.logger.info("  üß† Setting up intelligent cache system...")
            self.cache_system = get_intelligent_cache()
            
            # 4. Intelligent Auto-scaler
            self.logger.info("  ‚ö° Starting intelligent auto-scaler...")
            self.autoscaler = get_intelligent_autoscaler()
            self.autoscaler.start_intelligent_scaling()
            
            # 5. Advanced Telemetry
            self.logger.info("  üìà Launching advanced telemetry system...")
            self.telemetry = get_telemetry_system()
            self.telemetry.start_telemetry()
            
            self.logger.info("‚úÖ System initialization complete!")
            
            # Display system capabilities
            await self.display_system_capabilities()
            
        except Exception as e:
            self.logger.error(f"‚ùå System initialization failed: {e}")
            raise
            
    async def display_system_capabilities(self):
        """Display system capabilities and configuration."""
        self.logger.info("\n" + "="*80)
        self.logger.info("üéØ HIGH-PERFORMANCE NEUROMORPHIC VISION SYSTEM")
        self.logger.info("="*80)
        
        # GPU capabilities
        gpu_stats = self.gpu_processor.get_processing_stats()
        gpu_devices = gpu_stats['resource_stats']['total_devices']
        gpu_memory_gb = gpu_stats['resource_stats']['total_memory_mb'] / 1024
        
        self.logger.info(f"üñ•Ô∏è  GPU Processing:")
        self.logger.info(f"   ‚Ä¢ {gpu_devices} GPU device(s) available")
        self.logger.info(f"   ‚Ä¢ {gpu_memory_gb:.1f} GB total GPU memory")
        self.logger.info(f"   ‚Ä¢ CUDA kernel optimization enabled")
        self.logger.info(f"   ‚Ä¢ Mixed precision training enabled")
        
        # Pipeline capabilities
        pipeline_stats = self.async_pipeline.get_comprehensive_stats()
        worker_count = pipeline_stats['pipeline']['worker_count']
        buffer_capacity = 65536  # From pipeline config
        
        self.logger.info(f"‚ö° Async Event Pipeline:")
        self.logger.info(f"   ‚Ä¢ {worker_count} processing workers")
        self.logger.info(f"   ‚Ä¢ {buffer_capacity:,} event buffer capacity")
        self.logger.info(f"   ‚Ä¢ Lock-free data structures")
        self.logger.info(f"   ‚Ä¢ Sub-millisecond processing target")
        
        # Cache capabilities
        cache_stats = self.cache_system.get_comprehensive_stats()
        total_cache_gb = cache_stats['global']['total_capacity_mb'] / 1024
        
        self.logger.info(f"üß† Intelligent Cache System:")
        self.logger.info(f"   ‚Ä¢ {total_cache_gb:.1f} GB total cache capacity")
        self.logger.info(f"   ‚Ä¢ 3-level cache hierarchy (L1/L2/L3)")
        self.logger.info(f"   ‚Ä¢ Pattern recognition & predictive caching")
        self.logger.info(f"   ‚Ä¢ Adaptive eviction policies")
        
        # Auto-scaling capabilities
        scaler_stats = self.autoscaler.get_intelligent_scaling_stats()
        
        self.logger.info(f"üìà Intelligent Auto-scaling:")
        self.logger.info(f"   ‚Ä¢ ML-based workload prediction")
        self.logger.info(f"   ‚Ä¢ Proactive resource allocation")
        self.logger.info(f"   ‚Ä¢ Economic optimization")
        self.logger.info(f"   ‚Ä¢ Risk-aware scaling decisions")
        
        # Telemetry capabilities
        self.logger.info(f"üìä Advanced Telemetry:")
        self.logger.info(f"   ‚Ä¢ Sub-millisecond accuracy tracking")
        self.logger.info(f"   ‚Ä¢ Flame graph profiling")
        self.logger.info(f"   ‚Ä¢ Intelligent bottleneck detection")
        self.logger.info(f"   ‚Ä¢ Prometheus metrics integration")
        
        self.logger.info("="*80)
        
    async def run_performance_demonstration(self):
        """Run comprehensive performance demonstration."""
        self.logger.info("\nüé™ STARTING PERFORMANCE DEMONSTRATION")
        self.logger.info("="*60)
        
        # Phase 1: Baseline Performance Test
        await self.phase1_baseline_performance()
        
        # Phase 2: Stress Test with Auto-scaling
        await self.phase2_stress_test_autoscaling()
        
        # Phase 3: Cache Efficiency Demonstration
        await self.phase3_cache_efficiency()
        
        # Phase 4: Real-time Analytics
        await self.phase4_realtime_analytics()
        
        # Generate final performance report
        await self.generate_performance_report()
        
    async def phase1_baseline_performance(self):
        """Phase 1: Demonstrate baseline performance capabilities."""
        self.logger.info("\nüìä Phase 1: Baseline Performance Test")
        self.logger.info("-" * 40)
        
        phase_duration = 30.0
        target_rate = 100_000  # 100K events/sec to start
        
        # Generate synthetic event stream
        events_generated = 0
        events_processed = 0
        latency_measurements = []
        
        start_time = time.time()
        
        self.logger.info(f"Generating {target_rate:,} events/sec for {phase_duration}s...")
        
        while time.time() - start_time < phase_duration:
            # Generate batch of events
            batch_size = min(1000, int(target_rate * 0.01))  # 10ms worth
            events = self.generate_event_batch(batch_size)
            
            # Submit to async pipeline
            submitted = self.async_pipeline.submit_events_batch(events)
            events_generated += submitted
            
            # Collect results and measure latency
            for _ in range(submitted):
                result = self.async_pipeline.get_result(timeout=0.001)
                if result and not result.error:
                    events_processed += 1
                    # Calculate end-to-end latency
                    latency_ms = (result.completed_at - result.processing_time_ns / 1e9) * 1000
                    latency_measurements.append(latency_ms)
                    
            await asyncio.sleep(0.01)  # 10ms intervals
            
        # Calculate performance metrics
        actual_duration = time.time() - start_time
        actual_throughput = events_generated / actual_duration
        processing_throughput = events_processed / actual_duration
        
        if latency_measurements:
            avg_latency = np.mean(latency_measurements)
            p95_latency = np.percentile(latency_measurements, 95)
        else:
            avg_latency = p95_latency = 0
            
        self.demo_stats['events_generated'] += events_generated
        self.demo_stats['events_processed'] += events_processed
        self.demo_stats['peak_throughput_eps'] = max(
            self.demo_stats['peak_throughput_eps'], 
            processing_throughput
        )
        self.demo_stats['average_latency_ms'] = avg_latency
        
        self.logger.info(f"‚úÖ Phase 1 Results:")
        self.logger.info(f"   ‚Ä¢ Events Generated: {events_generated:,}")
        self.logger.info(f"   ‚Ä¢ Events Processed: {events_processed:,}")
        self.logger.info(f"   ‚Ä¢ Throughput: {processing_throughput:,.0f} events/sec")
        self.logger.info(f"   ‚Ä¢ Average Latency: {avg_latency:.3f} ms")
        self.logger.info(f"   ‚Ä¢ P95 Latency: {p95_latency:.3f} ms")
        
    async def phase2_stress_test_autoscaling(self):
        """Phase 2: Stress test with auto-scaling demonstration."""
        self.logger.info("\nüî• Phase 2: Stress Test with Auto-scaling")
        self.logger.info("-" * 40)
        
        phase_duration = 60.0
        initial_rate = 200_000  # 200K events/sec
        peak_rate = 1_000_000   # 1M events/sec peak
        
        start_time = time.time()
        initial_workers = self.autoscaler.current_workers
        
        self.logger.info(f"Ramping from {initial_rate:,} to {peak_rate:,} events/sec...")
        self.logger.info(f"Initial workers: {initial_workers}")
        
        scaling_events = []
        
        while time.time() - start_time < phase_duration:
            elapsed = time.time() - start_time
            progress = elapsed / phase_duration
            
            # Ramp up event rate
            current_rate = int(initial_rate + (peak_rate - initial_rate) * progress)
            
            # Generate events at current rate
            batch_size = min(2000, int(current_rate * 0.02))  # 20ms worth
            events = self.generate_event_batch(batch_size)
            
            # Submit events
            submitted = self.async_pipeline.submit_events_batch(events)
            self.demo_stats['events_generated'] += submitted
            
            # Check for scaling actions
            current_workers = self.autoscaler.current_workers
            if len(scaling_events) == 0 or scaling_events[-1]['workers'] != current_workers:
                scaling_events.append({
                    'time': elapsed,
                    'workers': current_workers,
                    'event_rate': current_rate
                })
                
            # Process results
            processed_count = 0
            for _ in range(10):  # Check for results
                result = self.async_pipeline.get_result(timeout=0.001)
                if result:
                    processed_count += 1
                else:
                    break
                    
            self.demo_stats['events_processed'] += processed_count
            
            await asyncio.sleep(0.02)  # 20ms intervals
            
        final_workers = self.autoscaler.current_workers
        self.demo_stats['scaling_actions'] = len(scaling_events) - 1
        
        # Calculate stress test metrics
        stress_duration = time.time() - start_time
        stress_throughput = self.demo_stats['events_generated'] / stress_duration
        
        self.logger.info(f"‚úÖ Phase 2 Results:")
        self.logger.info(f"   ‚Ä¢ Peak Event Rate: {peak_rate:,} events/sec")
        self.logger.info(f"   ‚Ä¢ Workers: {initial_workers} ‚Üí {final_workers}")
        self.logger.info(f"   ‚Ä¢ Scaling Actions: {self.demo_stats['scaling_actions']}")
        self.logger.info(f"   ‚Ä¢ Average Throughput: {stress_throughput:,.0f} events/sec")
        
        # Display scaling timeline
        self.logger.info(f"   ‚Ä¢ Scaling Timeline:")
        for event in scaling_events[:5]:  # Show first 5 scaling events
            self.logger.info(f"     - t={event['time']:.1f}s: {event['workers']} workers "
                           f"@ {event['event_rate']:,} events/sec")
            
    async def phase3_cache_efficiency(self):
        """Phase 3: Demonstrate cache efficiency."""
        self.logger.info("\nüß† Phase 3: Cache Efficiency Demonstration")
        self.logger.info("-" * 40)
        
        # Pre-populate cache with synthetic data patterns
        self.logger.info("Pre-populating cache with common patterns...")
        
        # Generate common event patterns
        pattern_data = {}
        for i in range(1000):
            # Create synthetic spatial patterns
            pattern_key = f"spatial_pattern_{i}"
            pattern_data[pattern_key] = np.random.random((100, 100))
            
            # Store in cache
            self.cache_system.put(pattern_key, pattern_data[pattern_key], tags={'pattern', 'spatial'})
            
        # Test cache performance with realistic access patterns
        cache_hits = 0
        cache_misses = 0
        access_times = []
        
        # Hot data access (80/20 rule)
        hot_keys = [f"spatial_pattern_{i}" for i in range(200)]  # Top 20%
        
        for _ in range(5000):  # 5000 cache accesses
            # 80% of accesses to hot data
            if np.random.random() < 0.8:
                key = np.random.choice(hot_keys)
            else:
                key = f"spatial_pattern_{np.random.randint(200, 1000)}"
                
            access_start = time.time_ns()
            value, hit = self.cache_system.get(key)
            access_time_us = (time.time_ns() - access_start) / 1000
            
            access_times.append(access_time_us)
            
            if hit:
                cache_hits += 1
            else:
                cache_misses += 1
                
        # Calculate cache metrics
        total_accesses = cache_hits + cache_misses
        hit_rate = cache_hits / total_accesses if total_accesses > 0 else 0
        avg_access_time = np.mean(access_times) if access_times else 0
        
        self.demo_stats['cache_hit_rate'] = hit_rate
        
        # Get comprehensive cache stats
        cache_stats = self.cache_system.get_comprehensive_stats()
        
        self.logger.info(f"‚úÖ Phase 3 Results:")
        self.logger.info(f"   ‚Ä¢ Cache Hit Rate: {hit_rate:.1%}")
        self.logger.info(f"   ‚Ä¢ Average Access Time: {avg_access_time:.1f} Œºs")
        self.logger.info(f"   ‚Ä¢ L1 Hit Rate: {cache_stats['levels']['L1'].get('hit_rate', 0):.1%}")
        self.logger.info(f"   ‚Ä¢ L2 Hit Rate: {cache_stats['levels']['L2'].get('hit_rate', 0):.1%}")
        self.logger.info(f"   ‚Ä¢ L3 Hit Rate: {cache_stats['levels']['L3'].get('hit_rate', 0):.1%}")
        self.logger.info(f"   ‚Ä¢ Cache Utilization: {cache_stats['global']['utilization_percent']:.1f}%")
        
    async def phase4_realtime_analytics(self):
        """Phase 4: Real-time analytics and monitoring."""
        self.logger.info("\nüìà Phase 4: Real-time Analytics")
        self.logger.info("-" * 40)
        
        # Generate comprehensive performance report
        performance_report = self.telemetry.generate_performance_report()
        
        # System health assessment
        health_score = performance_report['system_health_score']
        self.demo_stats['system_health_score'] = health_score
        
        # Bottleneck analysis
        bottlenecks = performance_report['performance_summary'].get('bottlenecks', {})
        active_bottlenecks = bottlenecks.get('active_count', 0)
        
        # Performance summary
        metrics = performance_report['performance_summary'].get('metric_summary', {})
        
        self.logger.info(f"‚úÖ Phase 4 Results:")
        self.logger.info(f"   ‚Ä¢ System Health Score: {health_score:.1f}/100")
        self.logger.info(f"   ‚Ä¢ Active Bottlenecks: {active_bottlenecks}")
        
        # Key performance indicators
        if 'cpu_utilization' in metrics:
            cpu_util = metrics['cpu_utilization']['current']
            self.logger.info(f"   ‚Ä¢ CPU Utilization: {cpu_util:.1f}%")
            
        if 'memory_utilization' in metrics:
            memory_util = metrics['memory_utilization']['current']
            self.logger.info(f"   ‚Ä¢ Memory Utilization: {memory_util:.1f}%")
            
        if 'event_throughput' in metrics:
            throughput = metrics['event_throughput']['current']
            self.logger.info(f"   ‚Ä¢ Current Throughput: {throughput:,.0f} events/sec")
            
        # Display hotspots if available
        hotspots = performance_report['performance_summary'].get('hotspots', {})
        if hotspots and hotspots.get('top_functions'):
            self.logger.info(f"   ‚Ä¢ Performance Hotspots:")
            for i, func in enumerate(hotspots['top_functions'][:3]):
                self.logger.info(f"     {i+1}. {func['function_name']}: "
                               f"{func['self_time_ms']:.3f} ms/call")
                
    def generate_event_batch(self, batch_size: int) -> List[NeuromorphicEvent]:
        """Generate batch of synthetic events."""
        events = []
        current_time = time.time()
        
        for i in range(batch_size):
            x = np.random.randint(0, self.event_resolution[0])
            y = np.random.randint(0, self.event_resolution[1])
            timestamp = current_time + (i * 1e-6)  # Microsecond spacing
            polarity = 1 if np.random.random() < 0.5 else -1
            
            event = NeuromorphicEvent(
                x=x, y=y, timestamp=timestamp, polarity=polarity,
                event_id=len(events), priority=EventPriority.MEDIUM
            )
            events.append(event)
            
        return events
        
    async def generate_performance_report(self):
        """Generate comprehensive performance report."""
        self.logger.info("\nüìã COMPREHENSIVE PERFORMANCE REPORT")
        self.logger.info("="*80)
        
        # Final system statistics
        total_events = self.demo_stats['events_generated']
        processed_events = self.demo_stats['events_processed']
        processing_rate = processed_events / total_events if total_events > 0 else 0
        
        self.logger.info(f"üéØ DEMONSTRATION SUMMARY:")
        self.logger.info(f"   ‚Ä¢ Total Events Generated: {total_events:,}")
        self.logger.info(f"   ‚Ä¢ Total Events Processed: {processed_events:,}")
        self.logger.info(f"   ‚Ä¢ Processing Success Rate: {processing_rate:.1%}")
        self.logger.info(f"   ‚Ä¢ Peak Throughput: {self.demo_stats['peak_throughput_eps']:,.0f} events/sec")
        self.logger.info(f"   ‚Ä¢ Average Latency: {self.demo_stats['average_latency_ms']:.3f} ms")
        self.logger.info(f"   ‚Ä¢ System Health Score: {self.demo_stats['system_health_score']:.1f}/100")
        
        # Component performance
        self.logger.info(f"\nüîß COMPONENT PERFORMANCE:")
        
        # GPU Processing
        gpu_stats = self.gpu_processor.get_processing_stats()
        self.logger.info(f"   GPU Processing:")
        self.logger.info(f"   ‚Ä¢ Tasks Completed: {gpu_stats['processing_stats']['tasks_completed']}")
        self.logger.info(f"   ‚Ä¢ GPU Utilization: {gpu_stats['resource_stats']['average_utilization']:.1f}%")
        
        # Cache System
        cache_stats = self.cache_system.get_comprehensive_stats()
        self.logger.info(f"   Cache System:")
        self.logger.info(f"   ‚Ä¢ Hit Rate: {self.demo_stats['cache_hit_rate']:.1%}")
        self.logger.info(f"   ‚Ä¢ Utilization: {cache_stats['global']['utilization_percent']:.1f}%")
        
        # Auto-scaling
        scaler_stats = self.autoscaler.get_intelligent_scaling_stats()
        self.logger.info(f"   Auto-scaling:")
        self.logger.info(f"   ‚Ä¢ Scaling Actions: {self.demo_stats['scaling_actions']}")
        self.logger.info(f"   ‚Ä¢ Decision Accuracy: {scaler_stats['intelligent_scaling']['average_decision_accuracy']:.1%}")
        
        # Performance targets assessment
        self.logger.info(f"\nüéØ PERFORMANCE TARGETS:")
        
        latency_target = self.demo_stats['average_latency_ms'] <= 1.0
        throughput_target = self.demo_stats['peak_throughput_eps'] >= 100_000
        health_target = self.demo_stats['system_health_score'] >= 80.0
        cache_target = self.demo_stats['cache_hit_rate'] >= 0.8
        
        self.logger.info(f"   ‚Ä¢ Sub-millisecond Latency: {'‚úÖ PASS' if latency_target else '‚ùå FAIL'}")
        self.logger.info(f"   ‚Ä¢ High Throughput (>100K eps): {'‚úÖ PASS' if throughput_target else '‚ùå FAIL'}")
        self.logger.info(f"   ‚Ä¢ System Health (>80): {'‚úÖ PASS' if health_target else '‚ùå FAIL'}")
        self.logger.info(f"   ‚Ä¢ Cache Efficiency (>80%): {'‚úÖ PASS' if cache_target else '‚ùå FAIL'}")
        
        overall_success = all([latency_target, throughput_target, health_target, cache_target])
        
        if overall_success:
            self.logger.info(f"\nüéâ DEMONSTRATION SUCCESSFUL!")
            self.logger.info(f"System meets all performance targets for high-throughput")
            self.logger.info(f"neuromorphic vision processing with sub-millisecond latency.")
        else:
            self.logger.info(f"\n‚ö†Ô∏è  SOME TARGETS NOT MET")
            self.logger.info(f"Review component performance for optimization opportunities.")
            
        # Save detailed report
        report_data = {
            'demonstration_summary': self.demo_stats,
            'gpu_processing': gpu_stats,
            'cache_system': cache_stats,
            'autoscaling': scaler_stats,
            'performance_targets': {
                'latency_target_met': latency_target,
                'throughput_target_met': throughput_target,
                'health_target_met': health_target,
                'cache_target_met': cache_target,
                'overall_success': overall_success
            },
            'timestamp': time.time()
        }
        
        report_path = Path('neuromorphic_system_demo_report.json')
        with open(report_path, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)
            
        self.logger.info(f"\nDetailed report saved to: {report_path}")
        
    async def shutdown_system(self):
        """Gracefully shutdown all system components."""
        self.logger.info("\nüõë Shutting down system components...")
        
        try:
            if self.telemetry:
                self.telemetry.stop_telemetry()
                
            if self.autoscaler:
                self.autoscaler.stop_intelligent_scaling()
                
            if self.async_pipeline:
                await self.async_pipeline.stop_pipeline()
                
            if self.gpu_processor:
                self.gpu_processor.stop_processing()
                
            if self.cache_system:
                self.cache_system.shutdown()
                
            self.logger.info("‚úÖ System shutdown complete")
            
        except Exception as e:
            self.logger.error(f"Error during shutdown: {e}")


async def run_comprehensive_benchmark():
    """Run comprehensive benchmark suite."""
    print("\n" + "="*80)
    print("üöÄ COMPREHENSIVE PERFORMANCE BENCHMARK SUITE")
    print("="*80)
    
    config = BenchmarkConfig(
        duration_seconds=30.0,  # Shorter for demo
        target_event_rate_eps=500_000,  # 500K events/sec
        max_latency_ms=1.0,
        min_throughput_eps=100_000,
        test_gpu_processing=True,
        test_async_pipeline=True,
        test_cache_system=True,
        test_auto_scaling=False,  # Skip for demo (takes too long)
        test_stress_conditions=True
    )
    
    suite = PerformanceBenchmarkSuite(config)
    results = await suite.run_all_benchmarks()
    
    return results


async def main():
    """Main demonstration execution."""
    print("üåü HIGH-PERFORMANCE NEUROMORPHIC VISION SYSTEM")
    print("   Advanced Performance Optimization & Auto-Scaling Demo")
    print("="*80)
    
    # Option 1: Full system demonstration
    demo = HighPerformanceNeuromorphicDemo()
    
    try:
        # Initialize all components
        await demo.initialize_system()
        
        # Run performance demonstration
        await demo.run_performance_demonstration()
        
        print("\n" + "="*80)
        print("‚ú® DEMONSTRATION COMPLETED SUCCESSFULLY!")
        print("="*80)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Demonstration interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Demonstration failed: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Ensure clean shutdown
        await demo.shutdown_system()
        
    print("\nüé≠ Optional: Run comprehensive benchmark suite? (uncomment below)")
    # Uncomment the following lines to run full benchmarks:
    # print("\n" + "="*80)
    # benchmark_results = await run_comprehensive_benchmark()
    
    print("\nüìã Demo complete! Check 'neuromorphic_system_demo_report.json' for detailed results.")


if __name__ == "__main__":
    # Set asyncio event loop policy for better performance
    try:
        import uvloop
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
    except ImportError:
        pass  # uvloop not available, use default policy
        
    # Run the demonstration
    asyncio.run(main())