#!/usr/bin/env python3
"""
Basic functionality test without heavy dependencies.
Tests core neuromorphic vision processing capabilities.
"""

import sys
import os
import time
import json
import random
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / 'src'))

def generate_synthetic_events(num_events: int = 1000) -> List[List[float]]:
    """Generate synthetic event data for testing."""
    events = []
    current_time = time.time()
    
    for i in range(num_events):
        event = [
            random.uniform(0, 128),    # x coordinate
            random.uniform(0, 128),    # y coordinate  
            current_time + i * 1e-4,   # timestamp
            random.choice([-1, 1])     # polarity
        ]
        events.append(event)
    
    return events

def basic_event_filter(events: List[List[float]]) -> List[List[float]]:
    """Basic event filtering without external dependencies."""
    filtered = []
    last_event_time = {}
    
    for event in events:
        x, y, t, p = event
        pixel_key = (int(x), int(y))
        
        # Simple refractory period filter
        if pixel_key in last_event_time:
            if t - last_event_time[pixel_key] < 1e-3:  # 1ms refractory
                continue
                
        # Basic bounds checking
        if not (0 <= x <= 128 and 0 <= y <= 128):
            continue
            
        filtered.append(event)
        last_event_time[pixel_key] = t
    
    return filtered

def simple_snn_inference(events: List[List[float]]) -> Dict[str, Any]:
    """Simplified SNN inference simulation."""
    start_time = time.time()
    
    # Simulate processing delay
    time.sleep(0.001)  # 1ms processing time
    
    # Generate mock detection results
    detections = []
    if len(events) > 100:  # Minimum events for detection
        num_detections = min(3, len(events) // 200)
        for i in range(num_detections):
            detections.append({
                'bbox': [
                    random.uniform(10, 100),
                    random.uniform(10, 100), 
                    random.uniform(20, 40),
                    random.uniform(20, 40)
                ],
                'confidence': random.uniform(0.5, 0.95),
                'class_id': random.randint(0, 2),
                'class_name': random.choice(['person', 'vehicle', 'object'])
            })
    
    inference_time = (time.time() - start_time) * 1000  # ms
    
    return {
        'detections': detections,
        'inference_time_ms': inference_time,
        'input_events': len(events),
        'processing_rate': len(events) / (inference_time / 1000) if inference_time > 0 else 0
    }

def test_event_processing_pipeline():
    """Test the complete event processing pipeline."""
    print("üß† Testing Event Processing Pipeline")
    print("=" * 50)
    
    # Generate test data
    print("üìä Generating synthetic event data...")
    raw_events = generate_synthetic_events(2000)
    print(f"   Generated {len(raw_events)} raw events")
    
    # Apply filtering
    print("üîç Applying event filtering...")
    filtered_events = basic_event_filter(raw_events)
    filter_ratio = len(filtered_events) / len(raw_events) * 100
    print(f"   Filtered to {len(filtered_events)} events ({filter_ratio:.1f}% passed)")
    
    # Run inference
    print("üöÄ Running SNN inference...")
    results = simple_snn_inference(filtered_events)
    
    print("\nüìà Results:")
    print(f"   Input events: {results['input_events']}")
    print(f"   Inference time: {results['inference_time_ms']:.2f} ms")
    print(f"   Processing rate: {results['processing_rate']:.0f} events/sec")
    print(f"   Detections found: {len(results['detections'])}")
    
    for i, detection in enumerate(results['detections']):
        bbox = detection['bbox']
        print(f"   Detection {i+1}: {detection['class_name']} "
              f"({detection['confidence']:.2f}) at [{bbox[0]:.1f}, {bbox[1]:.1f}, "
              f"{bbox[2]:.1f}, {bbox[3]:.1f}]")
    
    return results

def test_performance_benchmarks():
    """Test performance under various loads."""
    print("\n‚ö° Performance Benchmarks")
    print("=" * 50)
    
    test_sizes = [100, 500, 1000, 5000, 10000]
    results = []
    
    for size in test_sizes:
        print(f"Testing with {size} events...")
        events = generate_synthetic_events(size)
        filtered = basic_event_filter(events)
        
        start_time = time.time()
        result = simple_snn_inference(filtered)
        total_time = time.time() - start_time
        
        throughput = size / total_time
        results.append({
            'input_size': size,
            'throughput': throughput,
            'latency_ms': total_time * 1000,
            'detections': len(result['detections'])
        })
        
        print(f"   {throughput:.0f} events/sec, {total_time*1000:.2f} ms latency")
    
    # Performance analysis
    max_throughput = max(r['throughput'] for r in results)
    min_latency = min(r['latency_ms'] for r in results)
    
    print(f"\nüèÜ Peak Performance:")
    print(f"   Max throughput: {max_throughput:.0f} events/sec")
    print(f"   Min latency: {min_latency:.2f} ms")
    
    return results

def test_robustness():
    """Test system robustness with edge cases."""
    print("\nüõ°Ô∏è Robustness Testing")
    print("=" * 50)
    
    test_cases = [
        ("Empty events", []),
        ("Single event", [[64, 64, time.time(), 1]]),
        ("Out of bounds", [[200, 200, time.time(), 1]]),
        ("Negative coordinates", [[-10, -10, time.time(), -1]]),
        ("Very large dataset", generate_synthetic_events(50000))
    ]
    
    passed_tests = 0
    
    for test_name, events in test_cases:
        try:
            print(f"Testing: {test_name}")
            filtered = basic_event_filter(events)
            result = simple_snn_inference(filtered)
            
            print(f"   ‚úÖ PASSED - {len(filtered)} events processed, "
                  f"{len(result['detections'])} detections")
            passed_tests += 1
            
        except Exception as e:
            print(f"   ‚ùå FAILED - Error: {e}")
    
    success_rate = passed_tests / len(test_cases) * 100
    print(f"\nüìä Robustness Score: {success_rate:.1f}% ({passed_tests}/{len(test_cases)} tests passed)")
    
    return success_rate

def save_results(results: Dict[str, Any]):
    """Save test results to file."""
    results_file = Path(__file__).parent / 'test_results.json'
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüíæ Results saved to: {results_file}")

def main():
    """Main test execution."""
    print("üöÄ Photon Neuromorphics SDK - Basic Functionality Test")
    print("=" * 60)
    print(f"Python version: {sys.version}")
    print(f"Test started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    all_results = {}
    
    try:
        # Test core functionality
        pipeline_results = test_event_processing_pipeline()
        all_results['pipeline'] = pipeline_results
        
        # Performance benchmarks
        perf_results = test_performance_benchmarks()
        all_results['performance'] = perf_results
        
        # Robustness testing
        robustness_score = test_robustness()
        all_results['robustness_score'] = robustness_score
        
        # Overall assessment
        print("\nüéØ Overall Assessment")
        print("=" * 50)
        
        max_throughput = max(r['throughput'] for r in perf_results)
        avg_latency = sum(r['latency_ms'] for r in perf_results) / len(perf_results)
        
        if max_throughput > 10000 and avg_latency < 100 and robustness_score >= 80:
            status = "‚úÖ PRODUCTION READY"
        elif max_throughput > 5000 and avg_latency < 200 and robustness_score >= 60:
            status = "‚ö†Ô∏è  DEVELOPMENT READY"
        else:
            status = "‚ùå NEEDS IMPROVEMENT"
        
        print(f"Status: {status}")
        print(f"Max Throughput: {max_throughput:.0f} events/sec")
        print(f"Average Latency: {avg_latency:.2f} ms")
        print(f"Robustness Score: {robustness_score:.1f}%")
        
        all_results['overall_status'] = status
        all_results['summary'] = {
            'max_throughput': max_throughput,
            'avg_latency': avg_latency,
            'robustness_score': robustness_score
        }
        
        # Save results
        save_results(all_results)
        
    except Exception as e:
        print(f"\n‚ùå Test execution failed: {e}")
        return 1
    
    print("\nüéâ Testing completed successfully!")
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)