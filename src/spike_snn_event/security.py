"""
Security utilities and input sanitization for spike-snn-event-vision-kit.

Provides security hardening, input validation, and safe operation
practices for production deployment.
"""

import hashlib
import hmac
import secrets
import time
import logging
from typing import Dict, List, Any, Optional, Union, Callable
from pathlib import Path
import json
import pickle
from functools import wraps
import numpy as np
import torch
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import base64

from .validation import ValidationError, SpikeNNError


class SecurityError(SpikeNNError):
    """Raised when security violations are detected."""
    pass


class AuthenticationError(SecurityError):
    """Raised when authentication fails."""
    pass


class AuthorizationError(SecurityError):
    """Raised when authorization fails."""
    pass


class InputSanitizer:
    """Sanitizes and validates input data for security."""
    
    def __init__(self):\n        self.max_file_size_mb = 100\n        self.allowed_file_extensions = {'.npy', '.txt', '.h5', '.dat', '.json', '.yaml', '.yml'}\n        self.max_string_length = 1000\n        self.max_array_size = 1000000  # 1M elements\n        \n    def sanitize_file_path(self, file_path: Union[str, Path]) -> Path:\n        \"\"\"Sanitize file path to prevent directory traversal attacks.\"\"\"\n        path = Path(file_path)\n        \n        # Convert to absolute path and resolve any symlinks/.. sequences\n        try:\n            resolved_path = path.resolve()\n        except (OSError, ValueError) as e:\n            raise SecurityError(f\"Invalid file path: {e}\")\n        \n        # Check for directory traversal attempts\n        if '..' in str(path) or str(path).startswith('/'):\n            raise SecurityError(\"Directory traversal detected in file path\")\n            \n        # Check file extension\n        if resolved_path.suffix.lower() not in self.allowed_file_extensions:\n            raise SecurityError(f\"File extension '{resolved_path.suffix}' not allowed\")\n            \n        # Check file size if it exists\n        if resolved_path.exists():\n            file_size_mb = resolved_path.stat().st_size / (1024 * 1024)\n            if file_size_mb > self.max_file_size_mb:\n                raise SecurityError(f\"File too large: {file_size_mb:.1f}MB > {self.max_file_size_mb}MB\")\n                \n        return resolved_path\n        \n    def sanitize_string_input(self, input_str: str, field_name: str = \"input\") -> str:\n        \"\"\"Sanitize string input to prevent injection attacks.\"\"\"\n        if not isinstance(input_str, str):\n            raise SecurityError(f\"{field_name} must be a string\")\n            \n        if len(input_str) > self.max_string_length:\n            raise SecurityError(f\"{field_name} too long: {len(input_str)} > {self.max_string_length}\")\n            \n        # Remove null bytes and control characters\n        sanitized = ''.join(char for char in input_str if ord(char) >= 32 or char in '\\t\\n\\r')\n        \n        # Check for potential injection patterns\n        dangerous_patterns = [\n            '<script', '</script>', 'javascript:', 'eval(',\n            'exec(', '__import__', 'subprocess', 'os.system'\n        ]\n        \n        lower_input = sanitized.lower()\n        for pattern in dangerous_patterns:\n            if pattern in lower_input:\n                raise SecurityError(f\"Potentially dangerous pattern detected in {field_name}: {pattern}\")\n                \n        return sanitized\n        \n    def sanitize_numeric_input(\n        self, \n        value: Union[int, float], \n        min_val: Optional[float] = None,\n        max_val: Optional[float] = None,\n        field_name: str = \"input\"\n    ) -> Union[int, float]:\n        \"\"\"Sanitize numeric input.\"\"\"\n        if not isinstance(value, (int, float)):\n            raise SecurityError(f\"{field_name} must be numeric\")\n            \n        if not np.isfinite(value):\n            raise SecurityError(f\"{field_name} must be finite (not NaN or infinite)\")\n            \n        if min_val is not None and value < min_val:\n            raise SecurityError(f\"{field_name} below minimum: {value} < {min_val}\")\n            \n        if max_val is not None and value > max_val:\n            raise SecurityError(f\"{field_name} above maximum: {value} > {max_val}\")\n            \n        return value\n        \n    def sanitize_array_input(self, array: np.ndarray, field_name: str = \"array\") -> np.ndarray:\n        \"\"\"Sanitize numpy array input.\"\"\"\n        if not isinstance(array, np.ndarray):\n            raise SecurityError(f\"{field_name} must be numpy array\")\n            \n        if array.size > self.max_array_size:\n            raise SecurityError(f\"{field_name} too large: {array.size} > {self.max_array_size}\")\n            \n        # Check for invalid values\n        if not np.all(np.isfinite(array)):\n            raise SecurityError(f\"{field_name} contains non-finite values\")\n            \n        # Check data type safety\n        if array.dtype.kind in ['U', 'S', 'O']:  # Unicode, byte string, or object arrays\n            raise SecurityError(f\"{field_name} has unsafe data type: {array.dtype}\")\n            \n        return array\n        \n    def sanitize_dict_input(\n        self, \n        data: Dict[str, Any], \n        allowed_keys: Optional[List[str]] = None,\n        field_name: str = \"config\"\n    ) -> Dict[str, Any]:\n        \"\"\"Sanitize dictionary input.\"\"\"\n        if not isinstance(data, dict):\n            raise SecurityError(f\"{field_name} must be dictionary\")\n            \n        sanitized = {}\n        \n        for key, value in data.items():\n            # Sanitize key\n            clean_key = self.sanitize_string_input(str(key), f\"{field_name} key\")\n            \n            # Check if key is allowed\n            if allowed_keys is not None and clean_key not in allowed_keys:\n                raise SecurityError(f\"Key '{clean_key}' not allowed in {field_name}\")\n                \n            # Sanitize value based on type\n            if isinstance(value, str):\n                sanitized[clean_key] = self.sanitize_string_input(value, f\"{field_name}[{clean_key}]\")\n            elif isinstance(value, (int, float)):\n                sanitized[clean_key] = self.sanitize_numeric_input(value, field_name=f\"{field_name}[{clean_key}]\")\n            elif isinstance(value, (list, tuple)):\n                # Recursively sanitize list elements\n                sanitized[clean_key] = [self.sanitize_string_input(str(item)) if isinstance(item, str) else item for item in value]\n            else:\n                sanitized[clean_key] = value\n                \n        return sanitized\n\n\nclass SecureModelLoader:\n    \"\"\"Secure model loading with integrity verification.\"\"\"\n    \n    def __init__(self):\n        self.input_sanitizer = InputSanitizer()\n        self.trusted_checksums = {}  # Should be loaded from secure source\n        \n    def verify_model_integrity(self, model_path: Path, expected_checksum: Optional[str] = None) -> bool:\n        \"\"\"Verify model file integrity using checksum.\"\"\"\n        if not model_path.exists():\n            raise SecurityError(f\"Model file not found: {model_path}\")\n            \n        # Calculate file checksum\n        hasher = hashlib.sha256()\n        with open(model_path, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hasher.update(chunk)\n        actual_checksum = hasher.hexdigest()\n        \n        # Check against expected checksum\n        if expected_checksum:\n            if actual_checksum != expected_checksum:\n                raise SecurityError(\n                    f\"Model integrity check failed. \"\n                    f\"Expected: {expected_checksum}, Got: {actual_checksum}\"\n                )\n                \n        # Check against known trusted checksums\n        model_name = model_path.name\n        if model_name in self.trusted_checksums:\n            if actual_checksum not in self.trusted_checksums[model_name]:\n                raise SecurityError(f\"Model '{model_name}' checksum not in trusted list\")\n                \n        return True\n        \n    def safe_load_model(self, model_path: Union[str, Path], expected_checksum: Optional[str] = None):\n        \"\"\"Safely load model with security checks.\"\"\"\n        # Sanitize path\n        clean_path = self.input_sanitizer.sanitize_file_path(model_path)\n        \n        # Verify integrity\n        self.verify_model_integrity(clean_path, expected_checksum)\n        \n        # Check file extension for safe loading\n        if clean_path.suffix == '.pkl':\n            # Pickle files are dangerous - avoid or use restricted unpickler\n            raise SecurityError(\"Pickle files not allowed for security reasons\")\n        elif clean_path.suffix in ['.pth', '.pt']:\n            # PyTorch models - use safe loading\n            try:\n                return torch.load(clean_path, map_location='cpu', weights_only=True)\n            except Exception as e:\n                raise SecurityError(f\"Failed to load PyTorch model safely: {e}\")\n        else:\n            raise SecurityError(f\"Unsupported model format: {clean_path.suffix}\")\n\n\nclass RateLimiter:\n    \"\"\"Rate limiting for API endpoints and operations.\"\"\"\n    \n    def __init__(self, max_requests: int = 100, time_window: float = 60.0):\n        self.max_requests = max_requests\n        self.time_window = time_window\n        self.requests = {}  # client_id -> list of timestamps\n        \n    def is_allowed(self, client_id: str) -> bool:\n        \"\"\"Check if client is within rate limits.\"\"\"\n        current_time = time.time()\n        \n        # Clean old requests\n        if client_id in self.requests:\n            self.requests[client_id] = [\n                timestamp for timestamp in self.requests[client_id]\n                if current_time - timestamp < self.time_window\n            ]\n        else:\n            self.requests[client_id] = []\n            \n        # Check rate limit\n        if len(self.requests[client_id]) >= self.max_requests:\n            return False\n            \n        # Record new request\n        self.requests[client_id].append(current_time)\n        return True\n        \n    def get_reset_time(self, client_id: str) -> float:\n        \"\"\"Get time until rate limit resets for client.\"\"\"\n        if client_id not in self.requests or not self.requests[client_id]:\n            return 0.0\n            \n        oldest_request = min(self.requests[client_id])\n        return max(0.0, self.time_window - (time.time() - oldest_request))\n\n\nclass SecureConfig:\n    \"\"\"Secure configuration management with encryption.\"\"\"\n    \n    def __init__(self, password: Optional[str] = None):\n        self.input_sanitizer = InputSanitizer()\n        self.cipher = None\n        \n        if password:\n            self._setup_encryption(password)\n            \n    def _setup_encryption(self, password: str):\n        \"\"\"Setup encryption for sensitive configuration data.\"\"\"\n        # Derive key from password\n        password_bytes = password.encode()\n        salt = b'spike_snn_salt_2024'  # In production, use random salt per config\n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=32,\n            salt=salt,\n            iterations=100000,\n        )\n        key = base64.urlsafe_b64encode(kdf.derive(password_bytes))\n        self.cipher = Fernet(key)\n        \n    def encrypt_sensitive_data(self, data: str) -> str:\n        \"\"\"Encrypt sensitive configuration data.\"\"\"\n        if not self.cipher:\n            raise SecurityError(\"Encryption not configured\")\n            \n        return self.cipher.encrypt(data.encode()).decode()\n        \n    def decrypt_sensitive_data(self, encrypted_data: str) -> str:\n        \"\"\"Decrypt sensitive configuration data.\"\"\"\n        if not self.cipher:\n            raise SecurityError(\"Encryption not configured\")\n            \n        try:\n            return self.cipher.decrypt(encrypted_data.encode()).decode()\n        except Exception as e:\n            raise SecurityError(f\"Failed to decrypt data: {e}\")\n            \n    def load_secure_config(self, config_path: Union[str, Path]) -> Dict[str, Any]:\n        \"\"\"Load configuration with security validation.\"\"\"\n        # Sanitize path\n        clean_path = self.input_sanitizer.sanitize_file_path(config_path)\n        \n        # Load configuration\n        try:\n            with open(clean_path, 'r') as f:\n                if clean_path.suffix.lower() in ['.yaml', '.yml']:\n                    import yaml\n                    config = yaml.safe_load(f)\n                else:\n                    config = json.load(f)\n        except Exception as e:\n            raise SecurityError(f\"Failed to load configuration: {e}\")\n            \n        # Sanitize configuration\n        allowed_keys = [\n            'model_path', 'device', 'batch_size', 'learning_rate',\n            'epochs', 'threshold', 'input_width', 'input_height',\n            'num_classes', 'time_steps', 'integration_time',\n            'log_level', 'output_dir'\n        ]\n        \n        return self.input_sanitizer.sanitize_dict_input(config, allowed_keys, \"config\")\n        \n    def save_secure_config(self, config: Dict[str, Any], config_path: Union[str, Path]):\n        \"\"\"Save configuration securely.\"\"\"\n        # Sanitize path and config\n        clean_path = self.input_sanitizer.sanitize_file_path(config_path)\n        clean_config = self.input_sanitizer.sanitize_dict_input(config)\n        \n        # Encrypt sensitive fields if encryption is enabled\n        if self.cipher:\n            sensitive_fields = ['api_key', 'password', 'secret']\n            for field in sensitive_fields:\n                if field in clean_config:\n                    clean_config[f\"{field}_encrypted\"] = self.encrypt_sensitive_data(str(clean_config[field]))\n                    del clean_config[field]\n                    \n        # Save configuration\n        try:\n            with open(clean_path, 'w') as f:\n                if clean_path.suffix.lower() in ['.yaml', '.yml']:\n                    import yaml\n                    yaml.safe_dump(clean_config, f, default_flow_style=False)\n                else:\n                    json.dump(clean_config, f, indent=2)\n        except Exception as e:\n            raise SecurityError(f\"Failed to save configuration: {e}\")\n\n\ndef require_authentication(func: Callable) -> Callable:\n    \"\"\"Decorator to require authentication for function access.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # Simple token-based authentication\n        token = kwargs.pop('auth_token', None) or getattr(args[0], '_auth_token', None)\n        \n        if not token:\n            raise AuthenticationError(\"Authentication token required\")\n            \n        if not verify_token(token):\n            raise AuthenticationError(\"Invalid authentication token\")\n            \n        return func(*args, **kwargs)\n    return wrapper\n\n\ndef require_permission(permission: str) -> Callable:\n    \"\"\"Decorator to require specific permission for function access.\"\"\"\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            token = kwargs.pop('auth_token', None) or getattr(args[0], '_auth_token', None)\n            \n            if not token:\n                raise AuthenticationError(\"Authentication token required\")\n                \n            if not has_permission(token, permission):\n                raise AuthorizationError(f\"Permission '{permission}' required\")\n                \n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef rate_limit(max_requests: int = 100, time_window: float = 60.0) -> Callable:\n    \"\"\"Decorator to apply rate limiting to functions.\"\"\"\n    limiter = RateLimiter(max_requests, time_window)\n    \n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use IP address or user ID as client identifier\n            client_id = kwargs.pop('client_id', None) or 'default'\n            \n            if not limiter.is_allowed(client_id):\n                reset_time = limiter.get_reset_time(client_id)\n                raise SecurityError(\n                    f\"Rate limit exceeded. Try again in {reset_time:.1f} seconds\"\n                )\n                \n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef verify_token(token: str) -> bool:\n    \"\"\"Verify authentication token (placeholder implementation).\"\"\"\n    # In production, this would verify JWT tokens or check against a database\n    return len(token) >= 32 and token.isalnum()\n\n\ndef has_permission(token: str, permission: str) -> bool:\n    \"\"\"Check if token has specific permission (placeholder implementation).\"\"\"\n    # In production, this would check user roles and permissions\n    return verify_token(token)  # Simplified for example\n\n\ndef generate_secure_token() -> str:\n    \"\"\"Generate cryptographically secure token.\"\"\"\n    return secrets.token_urlsafe(32)\n\n\ndef hash_sensitive_data(data: str, salt: Optional[str] = None) -> tuple:\n    \"\"\"Hash sensitive data with salt.\"\"\"\n    if salt is None:\n        salt = secrets.token_hex(16)\n    \n    # Use PBKDF2 for password hashing\n    kdf = PBKDF2HMAC(\n        algorithm=hashes.SHA256(),\n        length=32,\n        salt=salt.encode(),\n        iterations=100000,\n    )\n    \n    hash_value = base64.b64encode(kdf.derive(data.encode())).decode()\n    return hash_value, salt\n\n\ndef verify_hash(data: str, hash_value: str, salt: str) -> bool:\n    \"\"\"Verify data against hash.\"\"\"\n    try:\n        computed_hash, _ = hash_sensitive_data(data, salt)\n        return hmac.compare_digest(hash_value, computed_hash)\n    except Exception:\n        return False\n\n\nclass SecurityAuditLog:\n    \"\"\"Security audit logging for tracking security events.\"\"\"\n    \n    def __init__(self, log_file: Optional[str] = None):\n        self.logger = logging.getLogger('security_audit')\n        self.logger.setLevel(logging.INFO)\n        \n        formatter = logging.Formatter(\n            '%(asctime)s - SECURITY - %(levelname)s - %(message)s'\n        )\n        \n        if log_file:\n            handler = logging.FileHandler(log_file)\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n            \n    def log_authentication_attempt(self, client_id: str, success: bool, details: str = \"\"):\n        \"\"\"Log authentication attempt.\"\"\"\n        status = \"SUCCESS\" if success else \"FAILURE\"\n        self.logger.info(f\"AUTH_{status} - Client: {client_id} - {details}\")\n        \n    def log_authorization_check(self, client_id: str, permission: str, granted: bool):\n        \"\"\"Log authorization check.\"\"\"\n        status = \"GRANTED\" if granted else \"DENIED\"\n        self.logger.info(f\"AUTHZ_{status} - Client: {client_id} - Permission: {permission}\")\n        \n    def log_security_violation(self, client_id: str, violation_type: str, details: str):\n        \"\"\"Log security violation.\"\"\"\n        self.logger.warning(f\"VIOLATION - Client: {client_id} - Type: {violation_type} - {details}\")\n        \n    def log_rate_limit_exceeded(self, client_id: str, endpoint: str):\n        \"\"\"Log rate limit violation.\"\"\"\n        self.logger.warning(f\"RATE_LIMIT - Client: {client_id} - Endpoint: {endpoint}\")\n\n\n# Global security instances\n_global_input_sanitizer = None\n_global_security_audit_log = None\n\n\ndef get_input_sanitizer() -> InputSanitizer:\n    \"\"\"Get global input sanitizer instance.\"\"\"\n    global _global_input_sanitizer\n    if _global_input_sanitizer is None:\n        _global_input_sanitizer = InputSanitizer()\n    return _global_input_sanitizer\n\n\ndef get_security_audit_log() -> SecurityAuditLog:\n    \"\"\"Get global security audit log instance.\"\"\"\n    global _global_security_audit_log\n    if _global_security_audit_log is None:\n        _global_security_audit_log = SecurityAuditLog()\n    return _global_security_audit_log