"""
Concurrent processing and resource pooling for spike-snn-event-vision-kit.

Provides thread pools, process pools, asynchronous processing, and resource
management for high-throughput neuromorphic vision processing.
"""

import asyncio
import threading
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Future, as_completed
from typing import Dict, List, Any, Optional, Callable, Union, Tuple, Iterator
from queue import Queue, PriorityQueue, Empty, Full
import time
import logging
import weakref
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from contextlib import contextmanager
import signal
import os
from pathlib import Path

try:
    import torch
    import torch.multiprocessing as torch_mp
    torch_mp.set_start_method('spawn', force=True)\n    TORCH_AVAILABLE = True\nexcept ImportError:\n    TORCH_AVAILABLE = False\n\nimport numpy as np\n\nfrom .monitoring import get_metrics_collector\nfrom .validation import ValidationError\n\n\n@dataclass\nclass TaskPriority:\n    \"\"\"Task priority levels.\"\"\"\n    CRITICAL: int = 1\n    HIGH: int = 2\n    NORMAL: int = 3\n    LOW: int = 4\n    BACKGROUND: int = 5\n\n\n@dataclass\nclass Task:\n    \"\"\"Task for concurrent processing.\"\"\"\n    id: str\n    func: Callable\n    args: tuple = field(default_factory=tuple)\n    kwargs: dict = field(default_factory=dict)\n    priority: int = TaskPriority.NORMAL\n    timeout: Optional[float] = None\n    callback: Optional[Callable] = None\n    created_at: float = field(default_factory=time.time)\n    \n    def __lt__(self, other):\n        # For priority queue ordering\n        if self.priority != other.priority:\n            return self.priority < other.priority\n        return self.created_at < other.created_at\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of task execution.\"\"\"\n    task_id: str\n    result: Any = None\n    error: Optional[Exception] = None\n    execution_time: float = 0.0\n    completed_at: float = field(default_factory=time.time)\n    \n    @property\n    def success(self) -> bool:\n        \"\"\"Check if task completed successfully.\"\"\"\n        return self.error is None\n\n\nclass ResourcePool(ABC):\n    \"\"\"Abstract base class for resource pools.\"\"\"\n    \n    @abstractmethod\n    def acquire(self, timeout: Optional[float] = None) -> Any:\n        \"\"\"Acquire resource from pool.\"\"\"\n        pass\n        \n    @abstractmethod\n    def release(self, resource: Any):\n        \"\"\"Release resource back to pool.\"\"\"\n        pass\n        \n    @abstractmethod\n    def size(self) -> int:\n        \"\"\"Get current pool size.\"\"\"\n        pass\n        \n    @abstractmethod\n    def available(self) -> int:\n        \"\"\"Get number of available resources.\"\"\"\n        pass\n\n\nclass ModelPool(ResourcePool):\n    \"\"\"Pool for managing neural network models.\"\"\"\n    \n    def __init__(self, model_factory: Callable, pool_size: int = 4):\n        self.model_factory = model_factory\n        self.pool_size = pool_size\n        self._pool = Queue(maxsize=pool_size)\n        self._lock = threading.Lock()\n        self._created_models = 0\n        self.logger = logging.getLogger(__name__)\n        \n        # Pre-populate pool\n        self._initialize_pool()\n        \n    def _initialize_pool(self):\n        \"\"\"Initialize pool with models.\"\"\"\n        for _ in range(self.pool_size):\n            try:\n                model = self.model_factory()\n                if TORCH_AVAILABLE and hasattr(model, 'eval'):\n                    model.eval()  # Set to evaluation mode\n                self._pool.put(model)\n                self._created_models += 1\n            except Exception as e:\n                self.logger.error(f\"Failed to create model for pool: {e}\")\n                break\n                \n        self.logger.info(f\"Initialized model pool with {self._created_models} models\")\n        \n    def acquire(self, timeout: Optional[float] = None) -> Any:\n        \"\"\"Acquire model from pool.\"\"\"\n        try:\n            model = self._pool.get(timeout=timeout)\n            return model\n        except Empty:\n            # If pool is empty and we haven't reached max size, create new model\n            with self._lock:\n                if self._created_models < self.pool_size:\n                    try:\n                        model = self.model_factory()\n                        if TORCH_AVAILABLE and hasattr(model, 'eval'):\n                            model.eval()\n                        self._created_models += 1\n                        return model\n                    except Exception as e:\n                        self.logger.error(f\"Failed to create model on demand: {e}\")\n                        \n            raise TimeoutError(\"No model available in pool\")\n            \n    def release(self, model: Any):\n        \"\"\"Release model back to pool.\"\"\"\n        try:\n            # Reset model state if needed\n            if TORCH_AVAILABLE and hasattr(model, 'eval'):\n                model.eval()\n                \n            self._pool.put_nowait(model)\n        except Full:\n            # Pool is full, just discard the model\n            pass\n            \n    def size(self) -> int:\n        \"\"\"Get current pool size.\"\"\"\n        return self._created_models\n        \n    def available(self) -> int:\n        \"\"\"Get number of available models.\"\"\"\n        return self._pool.qsize()\n        \n    @contextmanager\n    def get_model(self, timeout: Optional[float] = None):\n        \"\"\"Context manager for acquiring and releasing models.\"\"\"\n        model = self.acquire(timeout)\n        try:\n            yield model\n        finally:\n            self.release(model)\n\n\nclass WorkerProcess:\n    \"\"\"Worker process for CPU-intensive tasks.\"\"\"\n    \n    def __init__(self, worker_id: int, task_queue: mp.Queue, result_queue: mp.Queue):\n        self.worker_id = worker_id\n        self.task_queue = task_queue\n        self.result_queue = result_queue\n        self.process = None\n        self.is_running = False\n        \n    def start(self):\n        \"\"\"Start worker process.\"\"\"\n        if self.is_running:\n            return\n            \n        self.process = mp.Process(\n            target=self._worker_loop,\n            args=(self.worker_id, self.task_queue, self.result_queue)\n        )\n        self.process.start()\n        self.is_running = True\n        \n    def stop(self, timeout: float = 5.0):\n        \"\"\"Stop worker process.\"\"\"\n        if not self.is_running or not self.process:\n            return\n            \n        self.process.terminate()\n        self.process.join(timeout=timeout)\n        \n        if self.process.is_alive():\n            self.process.kill()\n            self.process.join()\n            \n        self.is_running = False\n        \n    @staticmethod\n    def _worker_loop(worker_id: int, task_queue: mp.Queue, result_queue: mp.Queue):\n        \"\"\"Main worker loop.\"\"\"\n        # Set up signal handlers for graceful shutdown\n        def signal_handler(signum, frame):\n            return\n            \n        signal.signal(signal.SIGTERM, signal_handler)\n        signal.signal(signal.SIGINT, signal_handler)\n        \n        while True:\n            try:\n                # Get task from queue\n                task = task_queue.get(timeout=1.0)\n                \n                if task is None:  # Shutdown signal\n                    break\n                    \n                # Execute task\n                start_time = time.time()\n                try:\n                    result = task.func(*task.args, **task.kwargs)\n                    task_result = TaskResult(\n                        task_id=task.id,\n                        result=result,\n                        execution_time=time.time() - start_time\n                    )\n                except Exception as e:\n                    task_result = TaskResult(\n                        task_id=task.id,\n                        error=e,\n                        execution_time=time.time() - start_time\n                    )\n                    \n                # Put result back\n                result_queue.put(task_result)\n                \n                # Call callback if provided\n                if task.callback:\n                    try:\n                        task.callback(task_result)\n                    except Exception:\n                        pass  # Ignore callback errors\n                        \n            except Empty:\n                continue\n            except Exception as e:\n                # Log error and continue\n                continue\n\n\nclass ConcurrentProcessor:\n    \"\"\"High-level concurrent processor with multiple execution strategies.\"\"\"\n    \n    def __init__(\n        self,\n        max_threads: int = 8,\n        max_processes: int = 4,\n        enable_gpu: bool = True\n    ):\n        self.max_threads = max_threads\n        self.max_processes = max_processes\n        self.enable_gpu = enable_gpu and TORCH_AVAILABLE\n        \n        # Executors\n        self.thread_executor = ThreadPoolExecutor(max_workers=max_threads)\n        self.process_executor = ProcessPoolExecutor(max_workers=max_processes)\n        \n        # Task management\n        self.task_queue = PriorityQueue()\n        self.pending_tasks: Dict[str, Future] = {}\n        self.completed_tasks: Dict[str, TaskResult] = {}\n        \n        # Worker threads\n        self.worker_threads = []\n        self.is_running = False\n        \n        # Resource pools\n        self.resource_pools: Dict[str, ResourcePool] = {}\n        \n        # Metrics\n        self.metrics = get_metrics_collector()\n        self.logger = logging.getLogger(__name__)\n        \n    def start(self):\n        \"\"\"Start concurrent processing.\"\"\"\n        if self.is_running:\n            return\n            \n        self.is_running = True\n        \n        # Start task dispatcher threads\n        for i in range(2):  # 2 dispatcher threads\n            worker = threading.Thread(\n                target=self._task_dispatcher_loop,\n                daemon=True\n            )\n            worker.start()\n            self.worker_threads.append(worker)\n            \n        self.logger.info(\"Concurrent processor started\")\n        \n    def stop(self, timeout: float = 30.0):\n        \"\"\"Stop concurrent processing.\"\"\"\n        if not self.is_running:\n            return\n            \n        self.is_running = False\n        \n        # Shutdown executors\n        self.thread_executor.shutdown(wait=False, cancel_futures=True)\n        self.process_executor.shutdown(wait=False, cancel_futures=True)\n        \n        # Wait for workers to finish\n        for worker in self.worker_threads:\n            worker.join(timeout=timeout / len(self.worker_threads))\n            \n        self.logger.info(\"Concurrent processor stopped\")\n        \n    def submit_task(\n        self,\n        task_id: str,\n        func: Callable,\n        *args,\n        priority: int = TaskPriority.NORMAL,\n        execution_mode: str = \"thread\",\n        timeout: Optional[float] = None,\n        callback: Optional[Callable] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"Submit task for concurrent execution.\"\"\"\n        \n        task = Task(\n            id=task_id,\n            func=func,\n            args=args,\n            kwargs=kwargs,\n            priority=priority,\n            timeout=timeout,\n            callback=callback\n        )\n        \n        # Add execution mode to task\n        task.execution_mode = execution_mode\n        \n        self.task_queue.put(task)\n        self.logger.debug(f\"Submitted task {task_id} with priority {priority}\")\n        \n        return task_id\n        \n    def submit_batch(\n        self,\n        tasks: List[Tuple[str, Callable, tuple, dict]],\n        priority: int = TaskPriority.NORMAL,\n        execution_mode: str = \"thread\"\n    ) -> List[str]:\n        \"\"\"Submit batch of tasks.\"\"\"\n        task_ids = []\n        \n        for task_id, func, args, kwargs in tasks:\n            submitted_id = self.submit_task(\n                task_id, func, *args,\n                priority=priority,\n                execution_mode=execution_mode,\n                **kwargs\n            )\n            task_ids.append(submitted_id)\n            \n        return task_ids\n        \n    def get_result(self, task_id: str, timeout: Optional[float] = None) -> TaskResult:\n        \"\"\"Get result of completed task.\"\"\"\n        # Check completed tasks first\n        if task_id in self.completed_tasks:\n            return self.completed_tasks.pop(task_id)\n            \n        # Check pending tasks\n        if task_id in self.pending_tasks:\n            future = self.pending_tasks[task_id]\n            try:\n                result = future.result(timeout=timeout)\n                task_result = TaskResult(task_id=task_id, result=result)\n                return task_result\n            except Exception as e:\n                task_result = TaskResult(task_id=task_id, error=e)\n                return task_result\n            finally:\n                self.pending_tasks.pop(task_id, None)\n                \n        raise KeyError(f\"Task {task_id} not found\")\n        \n    def wait_for_completion(self, task_ids: List[str], timeout: Optional[float] = None) -> Dict[str, TaskResult]:\n        \"\"\"Wait for multiple tasks to complete.\"\"\"\n        results = {}\n        start_time = time.time()\n        \n        for task_id in task_ids:\n            remaining_timeout = None\n            if timeout is not None:\n                elapsed = time.time() - start_time\n                remaining_timeout = max(0, timeout - elapsed)\n                \n            try:\n                result = self.get_result(task_id, remaining_timeout)\n                results[task_id] = result\n            except Exception as e:\n                results[task_id] = TaskResult(task_id=task_id, error=e)\n                \n        return results\n        \n    def _task_dispatcher_loop(self):\n        \"\"\"Main task dispatcher loop.\"\"\"\n        while self.is_running:\n            try:\n                # Get task from queue\n                task = self.task_queue.get(timeout=1.0)\n                \n                # Execute based on mode\n                execution_mode = getattr(task, 'execution_mode', 'thread')\n                \n                if execution_mode == \"thread\":\n                    future = self.thread_executor.submit(\n                        self._execute_task_with_timeout, task\n                    )\n                elif execution_mode == \"process\":\n                    future = self.process_executor.submit(\n                        self._execute_task_with_timeout, task\n                    )\n                else:\n                    # Direct execution\n                    try:\n                        result = self._execute_task_with_timeout(task)\n                        self.completed_tasks[task.id] = result\n                        continue\n                    except Exception as e:\n                        self.completed_tasks[task.id] = TaskResult(\n                            task_id=task.id, error=e\n                        )\n                        continue\n                        \n                self.pending_tasks[task.id] = future\n                \n                # Add completion callback\n                future.add_done_callback(\n                    lambda f, tid=task.id: self._handle_task_completion(tid, f)\n                )\n                \n            except Empty:\n                continue\n            except Exception as e:\n                self.logger.error(f\"Task dispatcher error: {e}\")\n                \n    def _execute_task_with_timeout(self, task: Task) -> TaskResult:\n        \"\"\"Execute task with timeout handling.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Set up timeout if specified\n            if task.timeout:\n                def timeout_handler(signum, frame):\n                    raise TimeoutError(f\"Task {task.id} timed out\")\n                    \n                old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n                signal.alarm(int(task.timeout))\n                \n            try:\n                result = task.func(*task.args, **task.kwargs)\n                \n                # Record success metrics\n                execution_time = time.time() - start_time\n                self.metrics.record_inference_latency(execution_time * 1000)\n                \n                return TaskResult(\n                    task_id=task.id,\n                    result=result,\n                    execution_time=execution_time\n                )\n                \n            finally:\n                if task.timeout:\n                    signal.alarm(0)\n                    signal.signal(signal.SIGALRM, old_handler)\n                    \n        except Exception as e:\n            execution_time = time.time() - start_time\n            self.metrics.record_error(\"task_execution\")\n            \n            return TaskResult(\n                task_id=task.id,\n                error=e,\n                execution_time=execution_time\n            )\n            \n    def _handle_task_completion(self, task_id: str, future: Future):\n        \"\"\"Handle task completion.\"\"\"\n        try:\n            result = future.result()\n            self.completed_tasks[task_id] = result\n            \n            # Execute callback if present\n            if hasattr(result, 'callback') and result.callback:\n                try:\n                    result.callback(result)\n                except Exception as e:\n                    self.logger.error(f\"Callback error for task {task_id}: {e}\")\n                    \n        except Exception as e:\n            self.completed_tasks[task_id] = TaskResult(task_id=task_id, error=e)\n            \n        # Clean up\n        self.pending_tasks.pop(task_id, None)\n        \n    def add_resource_pool(self, name: str, pool: ResourcePool):\n        \"\"\"Add resource pool.\"\"\"\n        self.resource_pools[name] = pool\n        \n    def get_resource_pool(self, name: str) -> Optional[ResourcePool]:\n        \"\"\"Get resource pool by name.\"\"\"\n        return self.resource_pools.get(name)\n        \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get processor statistics.\"\"\"\n        return {\n            'pending_tasks': len(self.pending_tasks),\n            'completed_tasks': len(self.completed_tasks),\n            'queue_size': self.task_queue.qsize(),\n            'thread_pool_active': getattr(self.thread_executor._threads, '__len__', lambda: 0)(),\n            'process_pool_active': len(getattr(self.process_executor._processes, 'keys', lambda: [])()),\n            'resource_pools': {\n                name: {'size': pool.size(), 'available': pool.available()}\n                for name, pool in self.resource_pools.items()\n            }\n        }\n\n\nclass AsyncProcessor:\n    \"\"\"Asynchronous processor using asyncio for I/O-bound tasks.\"\"\"\n    \n    def __init__(self, max_concurrent: int = 100):\n        self.max_concurrent = max_concurrent\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n        self.tasks: Dict[str, asyncio.Task] = {}\n        self.logger = logging.getLogger(__name__)\n        \n    async def submit_async_task(\n        self,\n        task_id: str,\n        coro: Callable,\n        *args,\n        **kwargs\n    ) -> str:\n        \"\"\"Submit asynchronous task.\"\"\"\n        async def wrapped_coro():\n            async with self.semaphore:\n                return await coro(*args, **kwargs)\n                \n        task = asyncio.create_task(wrapped_coro())\n        self.tasks[task_id] = task\n        \n        return task_id\n        \n    async def get_result_async(self, task_id: str) -> Any:\n        \"\"\"Get result of async task.\"\"\"\n        if task_id not in self.tasks:\n            raise KeyError(f\"Task {task_id} not found\")\n            \n        task = self.tasks.pop(task_id)\n        return await task\n        \n    async def wait_for_all(self, task_ids: List[str]) -> Dict[str, Any]:\n        \"\"\"Wait for all specified tasks to complete.\"\"\"\n        tasks_to_wait = []\n        for task_id in task_ids:\n            if task_id in self.tasks:\n                tasks_to_wait.append(self.tasks[task_id])\n                \n        if not tasks_to_wait:\n            return {}\n            \n        results = await asyncio.gather(*tasks_to_wait, return_exceptions=True)\n        \n        return dict(zip(task_ids, results))\n        \n    def cancel_task(self, task_id: str) -> bool:\n        \"\"\"Cancel async task.\"\"\"\n        if task_id in self.tasks:\n            task = self.tasks.pop(task_id)\n            return task.cancel()\n        return False\n        \n    def get_pending_count(self) -> int:\n        \"\"\"Get number of pending tasks.\"\"\"\n        return len(self.tasks)\n\n\nclass EventStreamProcessor:\n    \"\"\"Specialized processor for continuous event streams.\"\"\"\n    \n    def __init__(\n        self,\n        buffer_size: int = 10000,\n        batch_size: int = 100,\n        processing_interval: float = 0.01\n    ):\n        self.buffer_size = buffer_size\n        self.batch_size = batch_size\n        self.processing_interval = processing_interval\n        \n        # Event buffer\n        self.event_buffer = Queue(maxsize=buffer_size)\n        \n        # Processing\n        self.processor_thread = None\n        self.is_processing = False\n        self.process_func = None\n        \n        # Metrics\n        self.events_processed = 0\n        self.batches_processed = 0\n        self.processing_errors = 0\n        \n        self.logger = logging.getLogger(__name__)\n        \n    def start_processing(self, process_func: Callable):\n        \"\"\"Start event stream processing.\"\"\"\n        if self.is_processing:\n            return\n            \n        self.process_func = process_func\n        self.is_processing = True\n        \n        self.processor_thread = threading.Thread(\n            target=self._processing_loop,\n            daemon=True\n        )\n        self.processor_thread.start()\n        \n        self.logger.info(\"Event stream processing started\")\n        \n    def stop_processing(self):\n        \"\"\"Stop event stream processing.\"\"\"\n        self.is_processing = False\n        \n        if self.processor_thread:\n            self.processor_thread.join(timeout=5.0)\n            \n        self.logger.info(\"Event stream processing stopped\")\n        \n    def add_events(self, events: Union[np.ndarray, List[np.ndarray]]) -> bool:\n        \"\"\"Add events to processing buffer.\"\"\"\n        try:\n            if isinstance(events, list):\n                for event_batch in events:\n                    self.event_buffer.put_nowait(event_batch)\n            else:\n                self.event_buffer.put_nowait(events)\n            return True\n        except Full:\n            self.logger.warning(\"Event buffer full, dropping events\")\n            return False\n            \n    def _processing_loop(self):\n        \"\"\"Main processing loop for event stream.\"\"\"\n        batch = []\n        \n        while self.is_processing:\n            try:\n                # Collect batch\n                while len(batch) < self.batch_size and self.is_processing:\n                    try:\n                        events = self.event_buffer.get(timeout=self.processing_interval)\n                        batch.append(events)\n                    except Empty:\n                        break\n                        \n                if not batch:\n                    continue\n                    \n                # Process batch\n                try:\n                    if self.process_func:\n                        self.process_func(batch)\n                        \n                    self.events_processed += sum(len(events) for events in batch)\n                    self.batches_processed += 1\n                    \n                except Exception as e:\n                    self.processing_errors += 1\n                    self.logger.error(f\"Event processing error: {e}\")\n                    \n                batch.clear()\n                \n            except Exception as e:\n                self.logger.error(f\"Processing loop error: {e}\")\n                batch.clear()\n                \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get processing statistics.\"\"\"\n        return {\n            'events_processed': self.events_processed,\n            'batches_processed': self.batches_processed,\n            'processing_errors': self.processing_errors,\n            'buffer_size': self.event_buffer.qsize(),\n            'buffer_capacity': self.buffer_size\n        }\n\n\n# Global instances\n_global_concurrent_processor = None\n_global_async_processor = None\n\n\ndef get_concurrent_processor() -> ConcurrentProcessor:\n    \"\"\"Get global concurrent processor instance.\"\"\"\n    global _global_concurrent_processor\n    if _global_concurrent_processor is None:\n        _global_concurrent_processor = ConcurrentProcessor()\n        _global_concurrent_processor.start()\n    return _global_concurrent_processor\n\n\ndef get_async_processor() -> AsyncProcessor:\n    \"\"\"Get global async processor instance.\"\"\"\n    global _global_async_processor\n    if _global_async_processor is None:\n        _global_async_processor = AsyncProcessor()\n    return _global_async_processor\n\n\n# Utility functions\ndef parallel_map(\n    func: Callable,\n    iterable: List[Any],\n    max_workers: int = 8,\n    execution_mode: str = \"thread\"\n) -> List[Any]:\n    \"\"\"Parallel map function with configurable execution mode.\"\"\"\n    if execution_mode == \"thread\":\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            return list(executor.map(func, iterable))\n    elif execution_mode == \"process\":\n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            return list(executor.map(func, iterable))\n    else:\n        return [func(item) for item in iterable]\n\n\ndef parallel_batch_process(\n    func: Callable,\n    data_list: List[Any],\n    batch_size: int = 32,\n    max_workers: int = 4\n) -> List[Any]:\n    \"\"\"Process data in parallel batches.\"\"\"\n    # Split data into batches\n    batches = [data_list[i:i + batch_size] for i in range(0, len(data_list), batch_size)]\n    \n    # Process batches in parallel\n    def process_batch(batch):\n        return [func(item) for item in batch]\n        \n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        batch_results = list(executor.map(process_batch, batches))\n        \n    # Flatten results\n    results = []\n    for batch_result in batch_results:\n        results.extend(batch_result)\n        \n    return results